import numpy as np
import torch

print(torch.__version__)


scalar = torch.tensor(7)
scalar


scalar.ndim


scalar.shape


scalar.item()


vector = torch.tensor([6, 6])


vector


vector.ndim


vector.shape


matrix = torch.tensor([[7, 8], [9, 10]])


matrix.ndim


matrix


matrix[1]


matrix.shape


TENSOR = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])


TENSOR


TENSOR.shape


TENSOR.ndim


TENSOR[0]


TENSOR[0][2]


TENSOR.T


TENSOR.T.ndim


TENSOR.T.shape





random_tensor = torch.rand(3, 4)
random_tensor


random_tensor = torch.rand((3, 4))
random_tensor


random_image_size_tensor = torch.rand(size=(224, 224, 3))  # h, w, c
random_image_size_tensor.shape, random_image_size_tensor.ndim


zeros = torch.zeros(size=(3, 4))
zeros


ones = torch.ones(size=(3, 4))
ones


ones.dtype


torch.arange(0, 10)


one_to_ten = torch.arange(1, 11)
one_to_ten


one_to_thous = torch.arange(start=0, end=1000, step=66)
one_to_thous


ten_zeros = torch.zeros_like(input=one_to_ten)
ten_zeros


float_32_tensor = torch.tensor(
    [3.0, 6.0, 9.0], dtype=None, device=None, requires_grad=False
)
float_32_tensor


float_32_tensor.dtype


float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor


print(float_16_tensor.dtype, float_32_tensor.dtype)
(float_16_tensor * float_32_tensor)


int_32_tensor = torch.tensor([3, 6, 9], dtype=torch.int32)
int_32_tensor


float_32_tensor * int_32_tensor


long_tensor = torch.tensor([3, 6, 9], dtype=torch.long)
long_tensor


long_tensor.dtype


some_tensor = torch.rand((3, 4))
some_tensor


some_tensor.dtype, some_tensor.shape, some_tensor.device


some_tensor.size()


tensor = torch.tensor([1, 2, 3])
tensor


tensor + 10


tensor * 10


tensor - 10


torch.mul(tensor, 10)


tensor * tensor


tensor @ tensor


x = torch.arange(0, 100, 10)


x, x.dtype


x.max()


torch.mean(x, dtype=torch.float32)


x.type(torch.float32)


x.type(torch.float32).mean()


x.argmax()


x = torch.arange(1.0, 10.0)
x, x.shape


x_reshaped = x.reshape(1, 9)
x_reshaped


z = x.view((1, 9))
z, z.shape


z[:, 0] = 5
z


x


x_stacked = torch.stack([x, x, x], dim=1)
x_stacked


v_stack = torch.vstack([x, x])
v_stack


h_stack = torch.hstack([x, x])
h_stack


torch.squeeze(x)


x = torch.zeros(2, 1, 2, 1, 2)
x.shape


y = torch.squeeze(x)
y.size()


y = torch.squeeze(x, 0)
y.size()


y = torch.squeeze(x, 1)
y.size()


y = torch.squeeze(x, (1, 2, 3))
y.shape


x = torch.arange(1.0, 10.0)
x_reshaped = x.reshape(1, 9)
z = x.view((1, 9))
z, z.shape
z[:, 0] = 5
x_reshaped


x_reshaped.squeeze()


x_reshaped.squeeze().shape


x_reshaped


torch.unsqueeze(x_reshaped, 0)


x_original = torch.rand(size=(480, 640, 3))  # h, w, c

x_permuted = x_original.permute((2, 0, 1))  # c, h, w, 0 -> 1, 1 -> 2, 2 -> 0
x_permuted.shape


x_original.shape


x_original[0, 0, 0] = 6969


x_permuted


x = torch.arange(1, 10).reshape(1, 3, 3)
x


x[0]


x[0][0][0]


x[0][-1][-1]


x[0][2][2]


x[:, 0]


x[:, :, 1]


x[:, 2, 2]


array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array)
array, tensor


array[0] = 666


array


tensor





# torch.from_numpy(array).type(torch.float32)


array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array)
array = array + 1
array, tensor


array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array)
array += 1
array, tensor





tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor


tensor[0] = 666
tensor


numpy_tensor


random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)


RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3 ,4)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3, 4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)


torch.cuda.is_available()


device = 'cuda' if torch.cuda.is_available() else 'cpu'
device


torch.cuda.device_count()


tensor = torch.tensor([1,2,3])
tensor, tensor.device


tensor_on_gpu = tensor.to(device)
tensor_on_gpu


tensor_on_gpu.numpy()


tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu





x = torch.tensor([[1]])
x.item(), x.ndim


x = torch.tensor([1])
x.item(), x.ndim


x = torch.tensor([[[1]]])
x.item(), x.ndim


x = torch.tensor([[[1, 2, 3]]])
x.item(), x.ndim


x = torch.tensor([[1.0, -1.0], [1.0, 1.0]], requires_grad=True)
x


out = x.pow(2).sum()
out


out.backward()


out


x.grad


cuda = torch.device("cuda")
cuda0 = torch.device("cuda:0")


x = torch.tensor([1.0, 2.0], device=cuda0)


y = torch.tensor([1.0, 2.0]).cuda()


with torch.cuda.device(0):
    # allocates a tensor on GPU 1
    a = torch.tensor([1.0, 2.0], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1.0, 2.0]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1.0, 2.0]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda0)
    e = torch.randn(2).to(cuda0)
    f = torch.randn(2).cuda(cuda0)
    # d.device, e.device, and f.device are all device(type='cuda', index=2)

    print(d, e, f)


c















